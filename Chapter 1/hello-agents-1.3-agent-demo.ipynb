{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77396bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: tavily-python in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (0.7.21)\n",
      "Requirement already satisfied: openai in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from tavily-python) (0.12.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from httpx->tavily-python) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from tiktoken>=0.5.1->tavily-python) (2026.1.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomok\\desktop\\master research\\code\\langchain_env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# build a travel-planning agent \n",
    "# tavily-python: API for AI web search\n",
    "# openai: API for GPT calls\n",
    "\n",
    "#!pip install requests tavily-python openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39cb1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt for agent\n",
    "\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"\n",
    "你是一个智能旅行助手。你的任务是分析用户的请求，并使用可用工具一步步地解决问题。\n",
    "\n",
    "# 可用工具:\n",
    "- `get_weather(city: str)`: 查询指定城市的实时天气。\n",
    "- `get_attraction(city: str, weather: str)`: 根据城市和天气搜索推荐的旅游景点。\n",
    "\n",
    "# 输出格式要求:\n",
    "你的每次回复必须严格遵循以下格式，包含一对Thought和Action：\n",
    "\n",
    "Thought: [你的思考过程和下一步计划]\n",
    "Action: [你要执行的具体行动]\n",
    "\n",
    "Action的格式必须是以下之一：\n",
    "1. 调用工具：function_name(arg_name=\"arg_value\")\n",
    "2. 结束任务：Finish[最终答案]\n",
    "\n",
    "# 重要提示:\n",
    "- 每次只输出一对Thought-Action\n",
    "- Action必须在同一行，不要换行\n",
    "- 当收集到足够信息可以回答用户问题时，必须使用 Action: Finish[最终答案] 格式结束\n",
    "\n",
    "请开始吧！\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd33cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool_1 for weather search\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    call wttr.in API for real weather information\n",
    "    \"\"\"\n",
    "    \n",
    "    # API node, call for JSON data\n",
    "    url=f\"https://wttr.in/{city}?format=j1\"\n",
    "\n",
    "    try:\n",
    "        # send an http request\n",
    "        response = requests.get(url)\n",
    "        # check response state code is 200(success) or not\n",
    "        response.raise_for_status()\n",
    "        #get returned JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # parse current weather information\n",
    "        current_condition = data['current_condition'][0]\n",
    "        weather_desc = current_condition['weatherDesc'][0]['value']\n",
    "        temp_c = current_condition['temp_C']\n",
    "\n",
    "        # change format as natural language and return\n",
    "        return f\"{city}当前天气：{weather_desc}，气温{temp_c}摄氏度\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # handle network error\n",
    "        return f\"错误：查询天气时遇到网络问题-{e}\"\n",
    "    except (KeyError, IndexError) as e:\n",
    "        # handle data parsing error\n",
    "        return f\"错误：解析天气数据失败，可能是城市名称无效-{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1faf19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool_2 for tourist attraction web search\n",
    "\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "def get_attraction(city: str, weather: str) -> str:\n",
    "    \"\"\"\n",
    "    based on city and weather, search through Tavily Search API\n",
    "    and return tourist attraction recommendation\n",
    "    \"\"\"\n",
    "\n",
    "    # read API key from environment variable\n",
    "    api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"错误：未配置TAVILY_API_KEY环境变量\"\n",
    "    \n",
    "    # initialize Tavily client\n",
    "    tavily = TavilyClient(api_key=api_key)\n",
    "\n",
    "    # construct a query\n",
    "    query = f\"'{city}'在'{weather}'天气下最值得去的旅游景点推荐及理由\"\n",
    "\n",
    "    try:\n",
    "        # call API, include_answer=True will return a general answer\n",
    "        response = tavily.search(query=query, search_depth=\"basic\", include_answer=True)\n",
    "\n",
    "        # if Tavily returns a clean result to use directly\n",
    "        # response['answer'] is a general answer based on all search results\n",
    "        if response.get(\"answer\"):\n",
    "            return response[\"answer\"]\n",
    "        \n",
    "        # if no general answer, format original result\n",
    "        formatted_results = []\n",
    "        for result in response.get(\"results\", []):\n",
    "            formatted_results.append(f\"- {result['title']}: {result['content']}\")\n",
    "\n",
    "\n",
    "        if not formatted_results:\n",
    "            return \"抱歉，没有找到相关的旅游景点推荐。\"\n",
    "        \n",
    "        return \"根据搜索，为您找到一下信息:\\n\" + \"\\n\".join(formatted_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"错误：执行Tavily搜索时出现问题-{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c75bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate all tool functions into a dictionary for future calls\n",
    "available_tools = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_attraction\": get_attraction,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fc885df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design a openai-API-compatible client to call LLMs\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "class OpenAICompatibleClient:\n",
    "    \"\"\"\n",
    "    a client to call any LLM service that is compatible with OpenAI API\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, api_key: str, base_url: str):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    def generate(self, prompt: str, system_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        call LLM API to generate response\n",
    "        \"\"\"\n",
    "        print(\"正在调用大语言模型...\")\n",
    "        try:\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ]\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                stream=False\n",
    "            )\n",
    "            answer = response.choices[0].message.content\n",
    "            print(\"大语言模型响应成功。\")\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            print(f\"调用LLM API时发生错误：{e}\")\n",
    "            return \"错误：调用大语言模型服务时出错。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1b503f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign in ollama \n",
    "#!ollama signin\n",
    "#!ollama pull qwen3:8b\n",
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a8885ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入：你好，查询一下今天北京的天气，然后根据天气推荐一个合适的旅游景点\n",
      "========================================\n",
      "--- 循环 1 ---\n",
      "\n",
      "正在调用大语言模型...\n",
      "大语言模型响应成功。\n",
      "模型输出：\n",
      "Thought: 用户想要查询北京今天的天气，然后根据天气推荐景点。我需要先调用get_weather函数获取北京的实时天气信息，然后再根据天气情况调用get_attraction函数推荐合适的景点。\n",
      "\n",
      "Action: get_weather(city=\"北京\")\n",
      "\n",
      "Observation:错误：查询天气时遇到网络问题-HTTPSConnectionPool(host='wttr.in', port=443): Max retries exceeded with url: /%E5%8C%97%E4%BA%AC?format=j1 (Caused by ConnectTimeoutError(<HTTPSConnection(host='wttr.in', port=443) at 0x1ab559f4590>, 'Connection to wttr.in timed out. (connect timeout=None)'))\n",
      "========================================\n",
      "--- 循环 2 ---\n",
      "\n",
      "正在调用大语言模型...\n",
      "大语言模型响应成功。\n",
      "模型输出：\n",
      "Thought: 上次查询天气遇到网络超时，现在重新尝试调用天气查询功能获取北京的实时天气信息。\n",
      "\n",
      "Action: get_weather(city=\"北京\")\n",
      "\n",
      "Observation:北京当前天气：Sunny，气温12摄氏度\n",
      "========================================\n",
      "--- 循环 3 ---\n",
      "\n",
      "正在调用大语言模型...\n",
      "大语言模型响应成功。\n",
      "模型输出：\n",
      "Thought: 已成功获取北京今天的天气信息：天气晴朗，气温12摄氏度。现在我需要根据这个天气情况调用get_attraction函数来搜索适合的旅游景点推荐。\n",
      "\n",
      "Action: get_attraction(city=\"北京\", weather=\"Sunny\")\n",
      "\n",
      "Observation:在阳光明媚的北京天气下，故宫和颐和园是最值得去的旅游景点，前者展示历史建筑，后者有美丽的园林。\n",
      "========================================\n",
      "--- 循环 4 ---\n",
      "\n",
      "正在调用大语言模型...\n",
      "大语言模型响应成功。\n",
      "模型输出：\n",
      "Thought: 已成功获取北京今天的天气信息（Sunny，气温12摄氏度），并根据天气推荐了合适的旅游景点。现在我有了所有必要的信息，可以给用户提供完整的回答。\n",
      "\n",
      "Action: Finish[北京今天天气：晴朗，气温12摄氏度。根据今天的天气情况，我推荐您去故宫或颐和园参观：故宫展示了宏伟的历史建筑，颐和园则有美丽的园林风光。这两个景点都非常适合在晴朗的天气中游览。]\n",
      "\n",
      "任务完成，最终答案：<re.Match object; span=(0, 96), match='Finish[北京今天天气：晴朗，气温12摄氏度。根据今天的天气情况，我推荐您去故宫或颐和园参观：>\n"
     ]
    }
   ],
   "source": [
    "# integrate all components and execute planning with prompt-inserted LLMs\n",
    "\n",
    "import re\n",
    "\n",
    "# configurate LLM client\n",
    "# based on your service, replace \"YOUR_XXX\" with related api keys, url and model id\n",
    "API_KEY = \"sk-Vtue8QmtLrzwDsDL22E94e4aA0C243F3Be39Ae9b6844AfB4\"\n",
    "BASE_URL = \"https://aihubmix.com/v1\"\n",
    "MODEL_ID = \"glm-4.7-flash-free\"\n",
    "TAVILY_API_KEY = \"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\"\n",
    "os.environ['TAVILY_API_KEY'] = \"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\"\n",
    "\n",
    "llm = OpenAICompatibleClient(\n",
    "    model=MODEL_ID,\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "\n",
    "# initialization\n",
    "user_prompt = \"你好，查询一下今天北京的天气，然后根据天气推荐一个合适的旅游景点\"\n",
    "prompt_history = [f\"用户请求：{user_prompt}\"]\n",
    "\n",
    "print(f\"用户输入：{user_prompt}\\n\" + \"=\"*40)\n",
    "\n",
    "# execute main loop\n",
    "for i in range(5): # max loop counts\n",
    "    print(f\"--- 循环 {i+1} ---\\n\")\n",
    "\n",
    "    # build a prompt\n",
    "    full_prompt = \"\\n\".join(prompt_history)\n",
    "\n",
    "    # call LLM for reasoning\n",
    "    llm_output = llm.generate(full_prompt, system_prompt=AGENT_SYSTEM_PROMPT)\n",
    "    # LLM may output redundant Thought-Action, which needs to be truncated\n",
    "    match = re.search(r'(Thought:.*?Action:.*?)(?=\\n\\s*(?:Thought:|Action:|Observation:)|\\Z)', llm_output, re.DOTALL)\n",
    "    if match:\n",
    "        truncated = match.group(1).strip()\n",
    "        if truncated != llm_output.strip():\n",
    "            llm_output = truncated\n",
    "            print(\"已截断多余的 Thought-Action 对\")\n",
    "    print(f\"模型输出：\\n{llm_output}\\n\")\n",
    "    prompt_history.append(llm_output)\n",
    "\n",
    "    # parse and perform action\n",
    "    action_match = re.search(r\"Action:(.*)\", llm_output, re.DOTALL)\n",
    "    if not action_match:\n",
    "        observation = \"错误：未能解析到 Action 字段。请确保你的回复严格遵循 'Thought: ... Action: ...'的格式。\"\n",
    "        observation_str = f\"Observation: {observation}\"\n",
    "        print(f\"{observation_str}\\n\" + \"=\"*40)\n",
    "        prompt_history.append(observation_str)\n",
    "        continue\n",
    "    action_str = action_match.group(1).strip()\n",
    "\n",
    "    if action_str.startswith(\"Finish\"):\n",
    "        final_answer = re.match(r\"Finish\\[(.*)\\]\", action_str)\n",
    "        print(f\"任务完成，最终答案：{final_answer}\")\n",
    "        break\n",
    "\n",
    "    tool_name = re.search(r\"(\\w+)\\(\", action_str).group(1)\n",
    "    args_str = re.search(r\"\\((.*)\\)\", action_str).group(1)\n",
    "    kwargs = dict(re.findall(r'(\\w+)=\"([^\"]*)\"', args_str))\n",
    "\n",
    "    if tool_name in available_tools:\n",
    "        observation = available_tools[tool_name](**kwargs)\n",
    "    else:\n",
    "        observation = f\"错误：未定义的工具'{tool_name}'\"\n",
    "\n",
    "    # Record observation results\n",
    "    observation_str = f\"Observation:{observation}\"\n",
    "    print(f\"{observation_str}\\n\" + \"=\"*40)\n",
    "    prompt_history.append(observation_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
