{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4298145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy-agents Chapter 3 exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uninstall previous torch versions\n",
    "# !pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a891050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Hugging Face transformers and PyTorch with CUDA support\n",
    "# !pip install transformers \n",
    "# !pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6920031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311/311 [00:00<00:00, 974.05it/s, Materializing param=model.norm.weight]                               \n",
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca7acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¼–ç åçš„è¾“å…¥ç¤ºä¾‹ï¼š\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 108386,   3837, 109432, 107828,\n",
      "           1773, 151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# prepare dialogue input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"},\n",
    "]\n",
    "\n",
    "# format input for causal LM\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# encode input text\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"ç¼–ç åçš„è¾“å…¥ç¤ºä¾‹ï¼š\")\n",
    "print(model_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b259c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”Ÿæˆçš„å›å¤ï¼š\n",
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤è‡ªå·±æ˜¯ä»€ä¹ˆã€‚ä½œä¸ºAIåŠ©æ‰‹ï¼Œæˆ‘ä¸»è¦æä¾›å¸®åŠ©å’Œæ”¯æŒï¼Œä½†å…·ä½“ä¿¡æ¯å¯èƒ½éœ€è¦ç”¨æˆ·è¿›ä¸€æ­¥è¯´æ˜ã€‚æˆ‘åº”è¯¥ä¿æŒå‹å¥½å’Œå¼€æ”¾çš„æ€åº¦ï¼ŒåŒæ—¶ç¡®ä¿ä¿¡æ¯å‡†ç¡®ã€‚ç”¨æˆ·å¯èƒ½åªæ˜¯æƒ³äº†è§£æˆ‘çš„åŠŸèƒ½æˆ–èƒŒæ™¯ï¼Œæ‰€ä»¥éœ€è¦ç®€æ˜æ‰¼è¦åœ°å›ç­”ï¼Œé¿å…å†—é•¿ã€‚å¦å¤–ï¼Œè¦ç¡®ä¿å›ç­”ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ï¼Œå¯èƒ½è¿˜éœ€è¦è¯¢é—®æ˜¯å¦æœ‰å…¶ä»–é—®é¢˜éœ€è¦å¸®åŠ©ã€‚æœ€åï¼Œä¿æŒè¯­æ°”äº²åˆ‡è‡ªç„¶ï¼Œè®©ç”¨æˆ·æ„Ÿåˆ°è¢«é‡è§†ã€‚\n",
      "</think>\n",
      "\n",
      "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ï¼Œéšæ—¶ä¸ºæ‚¨æä¾›å¸®åŠ©å’Œæ”¯æŒã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨è§£å†³çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# generate response\n",
    "# max_new_tokens controls the length of the generated response\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# cut off the input part to get only the generated response\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# decode generated tokens to text\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\nç”Ÿæˆçš„å›å¤ï¼š\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
