## hello agents 习题练习

2026/02/10 ~ 

### Chapter 1

1. 请分析以下四个 `case` 中的**主体**是否属于智能体，如果是，那么属于哪种类型的智能体（可以从多个分类维度进行分析），并说明理由：

   `case A`：**一台符合冯·诺依曼结构的超级计算机**，拥有高达每秒 2EFlop 的峰值算力

   `case B`：**特斯拉自动驾驶系统**在高速公路上行驶时，突然检测到前方有障碍物，需要在毫秒级做出刹车或变道决策

   `case C`：**AlphaGo**在与人类棋手对弈时，需要评估当前局面并规划未来数十步的最优策略

   `case D`：**ChatGPT 扮演的智能客服**在处理用户投诉时，需要查询订单信息、分析问题原因、提供解决方案并安抚用户情绪

   > ans: 
   >
   > 知识点回顾：
   >
   > - 根据对智能体的定义，智能体是通过**传感器（sensor）**感知**环境（environment）**，并能够**自主地（autonomously）**执行**动作（action）**以达成特定目标的实体。
   > - PEAS任务环境定义：Performance, Environment, Actuators（执行器）, Sensors
   > - 对智能体的分类标准：内部决策架构复杂度，响应效率和规划性，知识表示
   >   - 内部决策架构复杂度：（从低到高）反馈响应器，世界模型
   >   - 响应效率与规划性：（两者一般存在trade-off）（追求效率的智能体，追求正确目标导向的智能体）温度平衡器，AlphaGo
   >   - 知识表示：（符号主义，亚符号主义，神经符号主义）知识工程，神经网络，LLM驱动的agent
   >
   > 
   >
   > 因此：
   >
   > case A: 不是智能体，因为它不具备**自主性**，而是依赖预定的程序驱动执行动作
   >
   > case B: 是智能体，属于模型复杂度高的，追求效率的，亚符号主义的神经网络智能体
   >
   > case C: 是智能体，属于模型复杂度高的，追求规划的，亚符号主义的神经网络智能体
   >
   > case D: 是智能体，属于模型复杂度高的，平衡效率和规划的，神经符号主义的agent智能体

2. 假设你需要为一个"智能健身教练"设计任务环境。这个智能体能够：

   - 通过可穿戴设备监测用户的心率、运动强度等生理数据
   - 根据用户的健身目标（减脂/增肌/提升耐力）动态调整训练计划
   - 在用户运动过程中提供实时语音指导和动作纠正
   - 评估训练效果并给出饮食建议

   请使用 PEAS 模型完整描述这个智能体的任务环境，并分析该环境具有哪些特性（如部分可观察、随机性、动态性等）。

   > |             | 描述                                                         | 特性                       |
   > | ----------- | ------------------------------------------------------------ | -------------------------- |
   > | Performance | 根据用户的健身目标，最优化训练计划，提供尽可能正确的实时语音指导和动作纠正，尽量准确评估训练效果，给出正确的饮食建议 |                            |
   > | Environment | 用户的心率，运动强度等生理数据                               | 部分可观性，随机性，动态性 |
   > | Actuators   | 生成和播放语音，显示格式化文本                               |                            |
   > | Sensors     | 记录用户生理数据，解析数据并传给决策模型                     |                            |
   >
   > 

3. 某电商公司正在考虑两种方案来处理售后退款申请：

   方案 A（`Workflow`）：设计一套固定流程，例如：

   A.1 对于一般商品且在 7 天之内，金额 `< 100RMB` 自动通过；`100-500RMB `由客服审核；`>500RMB` 需主管审批；而特殊商品（如定制品）一律拒绝退款

   A.2 对于超过 7 天的商品，无论金额，只能由客服审核或主管审批；

   方案 B（`Agent`）：搭建一个智能体系统，让它理解退款政策、分析用户历史行为、评估商品状况，并自主决策是否批准退款

   请分析：

   - 这两种方案各自的优缺点是什么？

   - 在什么情况下 `Workflow` 更合适？什么情况下 `Agent` 更有优势？如果你是该电商公司的负责人，你更倾向于采用哪种方案？
   - 是否存在一个方案 C，能够结合两种方案，达到扬长避短的效果？

   > 1. | 方案     | 优点                                         | 缺点                                                         |
   >    | -------- | -------------------------------------------- | ------------------------------------------------------------ |
   >    | workflow | 规则清晰，可控，易追溯，时间、资源效率高     | 缺乏处理例外的泛化能力，需要人为预先设计好流程，对于超过7天的商品需要人工介入审核或审批 |
   >    | agent    | 泛化能力强，不需要人工设计流程，无需人工介入 | 决策不可控，预期不确定，搭建和运行消耗时间和资源更多         |
   >
   > 2. 在目标明确，流程固定，没有或鲜有例外输入的条件下，workflow更合适（如报销流程，报税流程等固定手续）；在目标较开放，流程未知，任务环境复杂多变时，agent更合适。对于该案例，我会倾向于workflow方案
   >
   > 3. 方案C：保留A.1方案，对于A.2超过7天的商品，由方案B的智能体系统决策是否批准退款，综合这两种方案组成方案C。这在workflow高效的基础上，完全规避了人工介入，引入了部分灵活性。h

4. 在 1.3 节的智能旅行助手基础上，请思考如何添加以下功能（可以只描述设计思路，也可以进一步尝试代码实现）：

   > **提示**：思考如何修改 `Thought-Action-Observation` 循环来实现这些功能。

   - 添加一个"记忆"功能，让智能体记住用户的偏好（如喜欢历史文化景点、预算范围等）
   - 当推荐的景点门票已售罄时，智能体能够自动推荐备选方案
   - 如果用户连续拒绝了 3 个推荐，智能体能够反思并调整推荐策略

   > - 给出最终推荐景点以后，让用户反馈自己的偏好，存储反馈结果，在推荐的时候参考调用存储的用户偏好调整推荐结果
   > - 生成推荐景点以后，通过网站API查询推荐的景点门票存量，若已售罄，生成备选方案
   > - 给出最终推荐景点以后，让用户反馈是否拒绝，每次拒绝都生成备选方案，统计连续拒绝次数若达到3，则调整反思并调整推荐策略

5. 卡尼曼的"系统 1"（快速直觉）和"系统 2"（慢速推理）理论[2]为神经符号主义 AI 提供了很好的类比。请首先构思一个具体的智能体的落地应用场景，然后说明场景中的：

   > **提示**：医疗诊断助手、法律咨询机器人、金融风控系统等都是常见的应用场景

   - 哪些任务应该由"系统 1"处理？
   - 哪些任务应该由"系统 2"处理？
   - 这两个系统如何协同工作以达成最终目标？

   > 以医疗诊断助手为例：
   >
   > - 患者的身体生理特征识别和提取由系统1处理
   > - 综合已提取的特征推理出病症的种类和程度由系统2处理
   > - 系统1，系统2顺序处理：系统1识别提取生理特征，系统2分析特征推理病症

6. 尽管大语言模型驱动的智能体系统展现出了强大的能力，但它们仍然存在诸多局限。请分析以下问题：

   - 为什么智能体或智能体系统有时会产生"幻觉"（生成看似合理但实际错误的信息）？
   - 在 1.3 节的案例中，我们设置了最大循环次数为 5 次。如果没有这个限制，智能体可能会陷入什么问题？
   - 如何评估一个智能体的"智能"程度？仅使用准确率指标是否足够？

   > - 它们都基于LLM生成回答，而LLM的本质是归纳式的概率模型，以输出下一个最可能出现的词为准则生成文本，它并没有演绎推理的逻辑，而只是记住了多种模式
   > - 它可能会陷入出错-重试的死循环
   > - 只考虑准确率是不够的，还应当综合考虑召回率，F1 score（准确率和召回率的调和平均）是其中一种评价指标，另外，循环次数也可以纳入评价指标，完成相同的任务，循环次数越少，智能程度越高

### Chapter 2

...

### Chapter 3

1. 自然语言处理中，语言模型经历了从统计到神经网络的模型演进。

   - 请使用本章提供的迷你语料库（`datawhale agent learns`, `datawhale agent works`），计算句子 `agent works` 在Bigram模型下的概率
   - N-gram模型的核心假设是马尔可夫假设。请解释这个假设的含义，以及N-gram模型存在哪些根本性局限？
   - 神经网络语言模型（RNN/LSTM）和Transformer分别是如何克服N-gram模型局限的？它们各自的优势是什么？

   > - $$
   >   \begin{align}
   >   P(\text{agent works})
   >   & = P(\text{agent})P(\text{works|agent}) \\
   >   & = {2 \over 6}{1 \over 2} \\
   >   & = {1 \over 6}
   >   \end{align}
   >   $$
   >
   >   
   >
   > - N-gram模型中Markov假设的含义是对于一组序列中的每个词元，其条件概率最多只和它前面的n-1个词元有关。N-gram模型的根本性局限包括固定的窗口n不能捕捉超过n个词元长度的词元间关系；对相似语义的词汇和未出现在语料库中的词泛化能力差——它将词视作孤立、离散的符号，未能捕捉语义
   >
   > - 循环神经网络RNN/LSTM：通过词嵌入，构造语义空间；通过引入状态，为网络加入记忆，来捕捉任意长度的语义联系；优势是可以捕捉长序列的内部依赖关系
   >
   >   Transformer：词嵌入构造语义空间，注意力机制并行捕捉序列内部的依赖关系；优势是可以并行计算依赖关系

2. Transformer架构[4]是现代大语言模型的基础。其中：

   > **提示**：可以结合本章3.1.2节的代码实现来辅助理解

   - 自注意力机制（Self-Attention）的核心思想是什么？
   - 为什么Transformer能够并行处理序列，而RNN必须串行处理？位置编码（Positional Encoding）在其中起什么作用？
   - Decoder-Only架构与完整的Encoder-Decoder架构有什么区别？为什么现在主流的大语言模型都采用Decoder-Only架构？

   > - 自注意力的核心思想是处理每一个词时，都能关联它与序列中的任意一个词，并为它们分配独特的注意力权重，权重越大，代表它与这些词之间的关联性越强
   > - 因为Transformer的自注意力机制对于序列中每一个词的处理不依赖于对其他词的处理结果，具有独立性，注意力计算公式为简单的向量点乘，可以扩充为矩阵相乘达到并行处理；而RNN的每一个节点的记忆，都依赖于序列中上一个节点的处理结果决定，结构上规定了只能串行不能并行；位置编码为每个词附加了它在序列中位置的信息，弥补了自注意力机制对顺序无感的不足
   > - Decoder-Only架构和完整的Encoder-Decoder架构相比，舍去了Encoder，只保留了Decoder，通过Decoder自回归的方式预测下一个词。因为Decoder-Only这种架构的结构简单，易于复制，能够构建超大规模参数的模型，自回归生成机制也天然契合文本生成式任务

3. 文本子词分词算法是大语言模型的一项关键技术，负责将文本转换为模型可处理的 token 序列。那为什么不能直接以"字符"或"单词"作为模型的输入单元？BPE（Byte Pair Encoding）算法解决了什么问题？

   > 以字符作为模型的输入单元会额外增加模型从字符构建完整单词的负担，学习效率低下，以单词作为输入单元，由于语言中的词汇量很大，词表将变得臃肿庞大，词形相近的词之间的语义关系也难以捕捉。BPE算法是一种主流的子词分词算法，它解决了词表爆炸和捕捉词形相近语义联系的问题

4. 本章3.2.3节介绍了如何本地部署开源大语言模型。请完成以下实践和分析：

   > **提示**：这是一道动手实践题，建议实际操作

   - 按照本章的指导，在本地部署一个轻量级的开源模型（推荐[Qwen3-0.6B](https://modelscope.cn/models/Qwen/Qwen3-0.6B)），并尝试调整采样参数并观察其对输出的影响
   - 选择一个具体任务（如文本分类、信息抽取、代码生成等），设计并对比以下不同的提示策略（如Zero-shot、Few-shot、Chain-of-Thought）对输出结果的效果差异
   - 从性能、成本、可控性、隐私等维度比较闭源模型和开源模型
   - 如果你要构建一个企业级的客服智能体，你会选择哪种类型的模型？需要考虑哪些因素？

   > - |        | 闭源模型                                                     | 开源模型                             |
   >   | ------ | ------------------------------------------------------------ | ------------------------------------ |
   >   | 性能   | 在文本理解，代码生成，工具调用，逻辑推理，多模态生成等领域SOTA的还是闭源模型 | 部分模型在某些领域能力接近闭源       |
   >   | 成本   | 通过API调用云端模型，与token使用数量挂钩，无部署硬件成本     | 本地部署和运行所需硬件和运维开销     |
   >   | 可控性 | 低                                                           | 灵活性更高，提供定制化微调接口和工具 |
   >   | 隐私   | 低，数据会上传到服务提供商，泄露隐私                         | 本地部署条件下隐私保护高             |
   >
   > - 我会选择本地部署开源模型，考虑的因素如下：
   >
   >   - 客服智能体要求快速响应，选择响应速度快的模型
   >   - 企业级要求数据隐私保护达到标准，只能使用本地部署的模型以避免数据泄露

5. 模型幻觉（Hallucination）[11]是大语言模型当前存在的关键局限性之一。本章介绍了缓解幻觉的方法（如检索增强生成、多步推理、外部工具调用）

   - 请选择其中一种，说明其工作原理和适用场景
   - 调研前沿的研究和论文，是否还有其他的缓解模型幻觉的方法，他们又有哪些改进和优势？

   > - RAG：它的工作原理是构造一个知识库，在回答的时候先在知识库检索相关的内容，然后根据检索结果生成更可靠的回答，它适用于对文档进行整理，总结的自动化工作流
   > - 参考[Google Gemini](https://gemini.google.com/app/590f998304840cf7?hl=zh-cn)

6. 假设你要设计一个论文辅助阅读智能体，它能够帮助研究人员快速阅读并理解学术论文，包括：总结论文研究的核心内容、回答关于论文的问题、提取关键信息、比较多篇不同论文的观点等。请回答：

   - 你会选择哪个模型作为智能体设计时的基座模型？选择时需要考虑哪些因素？
   - 如何设计提示词来引导模型更好地理解学术论文？学术论文通常很长，可能超过模型的上下文窗口限制，你会如何解决这个问题？
   - 学术研究是严谨的，这意味着我们需要确保智能体生成的信息是准确客观忠于原文的。你认为系统中加入哪些设计能够更好的实现这一需求？

   > - 我会选择Gemini系列模型作为智能体设计时的基座模型。选择时我会考虑以下因素：
   >
   >   - 处理长文本上下文的能力。论文的文本数量较大，在比较多篇论文时，上下文会很长
   >   - 多模态信息的理解能力。论文常常包含图片和表格，模型需要同时理解文本和这些非文本内容
   >   - 成本。作为辅助阅读论文的智能体，开销不宜过大
   >   - 构造智能体工具链生态。有langchain等开发框架可降低构造智能体的难度
   >
   > - 提示工程：
   >
   >   - 指明llm扮演的角色：它充当一名论文辅助阅读智能体；
   >   - 明确它需要完成的任务：理解论文内容、总结论文研究的核心内容、回答关于论文的问题、提取关键信息、比较多篇不同论文的观点
   >
   >   解决上下文窗口瓶颈：
   >
   >   - 在system prompt里提示llm可以省略参考文献和致谢部分的内容
   >
   > - 加入RAG功能，让智能体把论文存储到向量数据库中，并根据需要用RAG来增强回答的真实性



### Chapter 4

1. 本章介绍了三种经典的智能体范式:`ReAct`、`Plan-and-Solve` 和 `Reflection`。请分析:

   - 这三种范式在"思考"与"行动"的组织方式上有什么本质区别？
   - 如果要设计一个"智能家居控制助手"（需要控制灯光、空调、窗帘等多个设备，并根据用户习惯自动调节），你会选择哪种范式作为基础架构？为什么？
   - 是否可以将这三种范式进行组合使用？若可以，请尝试设计一个混合范式的智能体架构，并说明其适用场景。

   > - |                | 组织方式                                   |
   >   | -------------- | ------------------------------------------ |
   >   | ReAct          | 同一个模型进行【思考-行动】循环，迭代优化  |
   >   | Plan-and-Solve | 先思考，再行动，一次过，没有迭代           |
   >   | Reflection     | 一个模型行动，另一个模型思考，循环迭代优化 |
   >
   > - 我会选择Reflection范式作为基础架构。因为它需要根据用户习惯自动调节，Reflection范式能够用memory存储用户的习惯反馈，据此优化agent的行为模式
   >
   > - |                | 优势                                 | 劣势                                         |
   >   | -------------- | ------------------------------------ | -------------------------------------------- |
   >   | ReAct          | 快速处理实时信息，有一定反馈调节能力 | 既是思考者又是执行者，严重依赖基座模型的能力 |
   >   | Plan-and-Solve | 处理需要长远规划的复杂的task         | 缺乏反馈调节能力                             |
   >   | Reflection     | 强反馈调节能力                       | 模型调用开销、提示工程复杂度倍增             |
   >
   >   基于以上特性分析，可以将三种范式组合起来使用，由Reflection范式作为主体，memory存储执行结果和反思反馈，待执行的task如果是简单的，调用ReAct Agent来执行，如果task是复杂的，调用Plan-and-Solve Agent来执行。适用场景：个人科研助手，承担论文总结，对比分析，实验设计等任务。论文总结和对比分析可以用ReAct模型执行，实验设计比较复杂，需要规划，可以交由Plan-and-Solve模型执行，执行结果和反馈存储到memory中，供后续参考

2. 在4.2节的 `ReAct` 实现中，我们使用了正则表达式来解析大语言模型的输出（如 `Thought` 和 `Action`）。请思考:

   - 当前的解析方法存在哪些潜在的脆弱性？在什么情况下可能会失败？
   - 除了正则表达式，还有哪些更鲁棒的输出解析方案？
   - 尝试修改本章的代码，使用一种更可靠的输出格式，并对比两种方案的优缺点

   > - _parse_output函数中Action的解析直接匹配到文本末尾，一旦模型出现幻觉生成了多对Thought-Action对，就会把后续的幻觉也包括进来，对Action内容的后续处理就会失败
   > - 生成JSON格式；使用模型内置的函数调用功能；使用成熟的框架解析如LangChain
   > - 

3. 工具调用是现代智能体的核心能力之一。基于4.2.2节的 `ToolExecutor` 设计，请完成以下扩展实践:

   > **提示**:这是一道动手实践题，建议实际编写代码

   - 为 `ReAct` 智能体添加一个"计算器"工具，使其能够处理复杂的数学计算问题（如"计算 `(123 + 456) × 789/ 12 = ?` 的结果"）
   - 设计并实现一个"工具选择失败"的处理机制:当智能体多次调用错误的工具或提供错误的参数时，系统应该如何引导它纠正？
   - 思考:如果可调用工具的数量增加到5050个甚至100100个，当前的工具描述方式是否还能有效工作？在可调用工具数量随业务需求显著增加时，从工程角度如何优化工具的组织和检索机制？

4. `Plan-and-Solve` 范式将任务分解为"规划"和"执行"两个阶段。请深入分析:

   - 在4.3节的实现中，规划阶段生成的计划是"静态"的（一次性生成，不可修改）。如果在执行过程中发现某个步骤无法完成或结果不符合预期，应该如何设计一个"动态重规划"机制？
   - 对比 `Plan-and-Solve` 与 `ReAct`:在处理"预订一次从北京到上海的商务旅行（包括机票、酒店、租车）"这样的任务时，哪种范式更合适？为什么？
   - 尝试设计一个"分层规划"系统:先生成高层次的抽象计划，然后针对每个高层步骤再生成详细的子计划。这种设计有什么优势？

5. `Reflection` 机制通过"执行-反思-优化"循环来提升输出质量。请思考:

   - 在4.4节的代码生成案例中，不同阶段使用的是同一个模型。如果使用两个不同的模型（例如，用一个更强大的模型来做反思，用一个更快的模型来做执行），会带来什么影响？
   - `Reflection` 机制的终止条件是"反馈中包含**无需改进**"或"达到最大迭代次数"。这种设计是否合理？能否设计一个更智能的终止条件？
   - 假设你要搭建一个"学术论文写作助手"，它能够生成初稿并不断优化论文内容。请设计一个多维度的Reflection机制，从段落逻辑性、方法创新性、语言表达、引用规范等多个角度进行反思和改进。

6. 提示词工程是影响智能体最终效果的关键技术。本章展示了多个精心设计的提示词模板。请分析:

   - 对比4.2.3节的 `ReAct` 提示词和4.3.2节的 `Plan-and-Solve` 提示词，它们显然存在结构设计上的明显不同，这些差异是如何服务于各自范式的核心逻辑的？
   - 在4.4.3节的 `Reflection` 提示词中，我们使用了"你是一位极其严格的代码评审专家"这样的角色设定。尝试修改这个角色设定（如改为"你是一位注重代码可读性的开源项目维护者"），观察输出结果的变化，并总结角色设定对智能体行为的影响。
   - 在提示词中加入 `few-shot` 示例往往能显著提升模型对特定格式的遵循能力。请为本章的某个智能体尝试添加 `few-shot` 示例，并对比其效果。

7. 某电商初创公司现在希望使用"客服智能体"来代替真人客服实现降本增效，它需要具备以下功能:

   a. 理解用户的退款申请理由

   b. 查询用户的订单信息和物流状态

   c. 根据公司政策智能地判断是否应该批准退款

   d. 生成一封得体的回复邮件并发送至用户邮箱

   e. 如果判断决策存在一定争议（自我置信度低于阈值），能够进行自我反思并给出更审慎的建议

   此时作为该产品的负责人:

   - 你会选择本章的哪种范式（或哪些范式的组合）作为系统的核心架构？
   - 这个系统需要哪些工具？请列出至少3个工具及其功能描述。
   - 如何设计提示词来确保智能体的决策既符合公司利益，又能保持对用户的友好态度？
   - 这个产品上线后可能面临哪些风险和挑战？如何通过技术手段来降低这些风险？